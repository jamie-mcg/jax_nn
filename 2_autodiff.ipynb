{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from jax import grad, jit, vmap, pmap  # type: ignore\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 16:55:20.303821: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.1 which is older than the PTX compiler version (12.5.82). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4 -4.8  2.4]\n"
     ]
    }
   ],
   "source": [
    "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
    "theta = jnp.array([0.1, -0.1, 0.])\n",
    "\n",
    "s_tm1 = jnp.array([1., 2., -1.])\n",
    "r_t = jnp.array(1.)\n",
    "s_t = jnp.array([2., 1., 0.])\n",
    "\n",
    "def loss_fn(theta, s_tm1, r_t, s_t):\n",
    "    stop_term = r_t + value_fn(theta, s_t)\n",
    "    return (jax.lax.stop_gradient(stop_term) - value_fn(theta, s_tm1))**2\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "update = grad_fn(theta, s_tm1, r_t, s_t)\n",
    "\n",
    "print(update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x):  6.0\n",
      "Straight through f(x):  6.0\n",
      "grad(f)(x):  0.0\n",
      "grad(straight_through_f)(x):  1.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return jnp.round(x)\n",
    "\n",
    "def straight_through_f(x):\n",
    "    return x + jax.lax.stop_gradient(f(x) - x)\n",
    "\n",
    "x = 5.6\n",
    "print(\"f(x): \", f(x))\n",
    "print(\"Straight through f(x): \", straight_through_f(x))\n",
    "\n",
    "print(\"grad(f)(x): \", jax.grad(f)(x))\n",
    "print(\"grad(straight_through_f)(x): \", jax.grad(straight_through_f)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-2.4, -4.8,  2.4],\n",
       "       [-2.4, -4.8,  2.4]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_s_tm1 = jnp.stack([s_tm1, s_tm1])\n",
    "batched_r_t = jnp.stack([r_t, r_t])\n",
    "batched_s_t = jnp.stack([s_t, s_t])\n",
    "\n",
    "example_grads = jax.jit(jax.vmap(jax.grad(loss_fn), in_axes=(None, 0, 0, 0)))\n",
    "example_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, data):\n",
    "    pass\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "def meta_loss_fn(params, data):\n",
    "    grads = jax.grad(loss_fn)(params, data)\n",
    "    return loss_fn(params - lr * grads, data)\n",
    "\n",
    "meta_grads = jax.grad(meta_loss_fn)(params, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-nn-dFg24gDm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
